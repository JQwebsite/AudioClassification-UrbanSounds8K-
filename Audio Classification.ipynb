{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcmtmsaxbwTy"
      },
      "source": [
        "**Download dataset from Kaggle**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gmWyqTs98kyQ",
        "outputId": "530bcedc-551f-4ee9-f4e2-018ce6093e41"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# files.upload()\n",
        "# ! pip install -q kaggle\n",
        "# ! mkdir ~/.kaggle\n",
        "# ! cp kaggle.json ~/.kaggle/\n",
        "# ! chmod 600 ~/.kaggle/kaggle.json\n",
        "# ! kaggle datasets download -d chrisfilo/urbansound8k\n",
        "# ! unzip urbansound8k.zip -d UrbanSounds8K"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmVjVvU3cYGZ"
      },
      "source": [
        "**Inspect the audio files**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WP4iapU3B5dH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torchaudio\n",
        "import IPython.display as ipd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Audio\n",
        "import random\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import torch\n",
        "import torchaudio\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, models, transforms\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "huXMRna2t-YC"
      },
      "outputs": [],
      "source": [
        "def show_spectrogram(audio):\n",
        "  # only shows first channel\n",
        "    spectrogram = torchaudio.transforms.Spectrogram()(audio)[0]\n",
        "    print(\"\\nShape of spectrogram: {}\".format(spectrogram.size()))\n",
        "    plt.imshow(spectrogram.log2().numpy(), cmap='viridis', origin='lower')\n",
        "    plt.show()\n",
        "\n",
        "def pad_trunc(aud, max_ms):\n",
        "    sig, sr = aud\n",
        "    num_rows, sig_len = sig.shape\n",
        "    max_len = sr//1000 * max_ms\n",
        "\n",
        "    if (sig_len > max_len):\n",
        "      # Truncate the signal to the given length\n",
        "      sig = sig[:,:max_len]\n",
        "\n",
        "    elif (sig_len < max_len):\n",
        "      # Length of padding to add at the beginning and end of the signal\n",
        "      pad_begin_len = random.randint(0, max_len - sig_len)\n",
        "      pad_end_len = max_len - sig_len - pad_begin_len\n",
        "\n",
        "      # Pad with 0s\n",
        "      pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
        "      pad_end = torch.zeros((num_rows, pad_end_len))\n",
        "\n",
        "      sig = torch.cat((pad_begin, sig, pad_end), 1)\n",
        "      \n",
        "    return (sig, sr)\n",
        "\n",
        "def rechannel(aud, new_channel):\n",
        "    sig, sr = aud\n",
        "\n",
        "    if (sig.shape[0] == new_channel):\n",
        "      # Nothing to do\n",
        "      return aud\n",
        "\n",
        "    if (new_channel == 1):\n",
        "      # Convert from stereo to mono by selecting only the first channel\n",
        "      resig = sig[:1, :]\n",
        "    else:\n",
        "      # Convert from mono to stereo by duplicating the first channel\n",
        "      resig = torch.cat([sig, sig])\n",
        "\n",
        "    return ((resig, sr))\n",
        "\n",
        "def resample(aud, newsr):\n",
        "    sig, sr = aud\n",
        "\n",
        "    if (sr == newsr):\n",
        "      # Nothing to do\n",
        "      return aud\n",
        "\n",
        "    num_channels = sig.shape[0]\n",
        "    # Resample first channel\n",
        "    resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1,:])\n",
        "    if (num_channels > 1):\n",
        "      # Resample the second channel and merge both channels\n",
        "      retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:,:])\n",
        "      resig = torch.cat([resig, retwo])\n",
        "\n",
        "    return ((resig, newsr))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1W6OFH5xYDJ"
      },
      "source": [
        "**Original Audio**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "pFJgdpckbhB5",
        "outputId": "c4236b0e-ef4d-4c76-f349-e62df95ec4dd"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Error opening './UrbanSounds8K/fold1/101415-3-0-2.wav': System error.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./UrbanSounds8K/fold1/101415-3-0-2.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m waveform, sample_rate \u001b[38;5;241m=\u001b[39m rechannel(\u001b[43mtorchaudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      3\u001b[0m fig, axs \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      4\u001b[0m axs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mplot(waveform\u001b[38;5;241m.\u001b[39mt()\u001b[38;5;241m.\u001b[39mnumpy())\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchaudio\\backend\\soundfile_backend.py:205\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filepath, frame_offset, num_frames, normalize, channels_first, format)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[39m@_mod_utils\u001b[39m\u001b[39m.\u001b[39mrequires_soundfile()\n\u001b[0;32m    124\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\n\u001b[0;32m    125\u001b[0m     filepath: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[39mformat\u001b[39m: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    131\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor, \u001b[39mint\u001b[39m]:\n\u001b[0;32m    132\u001b[0m     \u001b[39m\"\"\"Load audio data from file.\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \n\u001b[0;32m    134\u001b[0m \u001b[39m    Note:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39m            `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 205\u001b[0m     \u001b[39mwith\u001b[39;00m soundfile\u001b[39m.\u001b[39;49mSoundFile(filepath, \u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m file_:\n\u001b[0;32m    206\u001b[0m         \u001b[39mif\u001b[39;00m file_\u001b[39m.\u001b[39mformat \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mWAV\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m normalize:\n\u001b[0;32m    207\u001b[0m             dtype \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m\"\u001b[39m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\soundfile.py:629\u001b[0m, in \u001b[0;36mSoundFile.__init__\u001b[1;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mode \u001b[39m=\u001b[39m mode\n\u001b[0;32m    627\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info \u001b[39m=\u001b[39m _create_info_struct(file, mode, samplerate, channels,\n\u001b[0;32m    628\u001b[0m                                  \u001b[39mformat\u001b[39m, subtype, endian)\n\u001b[1;32m--> 629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_file \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open(file, mode_int, closefd)\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mset\u001b[39m(mode)\u001b[39m.\u001b[39missuperset(\u001b[39m'\u001b[39m\u001b[39mr+\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseekable():\n\u001b[0;32m    631\u001b[0m     \u001b[39m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseek(\u001b[39m0\u001b[39m)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\soundfile.py:1183\u001b[0m, in \u001b[0;36mSoundFile._open\u001b[1;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[0;32m   1181\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1182\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid file: \u001b[39m\u001b[39m{0!r}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname))\n\u001b[1;32m-> 1183\u001b[0m _error_check(_snd\u001b[39m.\u001b[39;49msf_error(file_ptr),\n\u001b[0;32m   1184\u001b[0m              \u001b[39m\"\u001b[39;49m\u001b[39mError opening \u001b[39;49m\u001b[39m{0!r}\u001b[39;49;00m\u001b[39m: \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mformat(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname))\n\u001b[0;32m   1185\u001b[0m \u001b[39mif\u001b[39;00m mode_int \u001b[39m==\u001b[39m _snd\u001b[39m.\u001b[39mSFM_WRITE:\n\u001b[0;32m   1186\u001b[0m     \u001b[39m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m     \u001b[39m# when opening a named pipe in SFM_WRITE mode.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m     \u001b[39m# See http://github.com/erikd/libsndfile/issues/77.\u001b[39;00m\n\u001b[0;32m   1189\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info\u001b[39m.\u001b[39mframes \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\soundfile.py:1357\u001b[0m, in \u001b[0;36m_error_check\u001b[1;34m(err, prefix)\u001b[0m\n\u001b[0;32m   1355\u001b[0m \u001b[39mif\u001b[39;00m err \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1356\u001b[0m     err_str \u001b[39m=\u001b[39m _snd\u001b[39m.\u001b[39msf_error_number(err)\n\u001b[1;32m-> 1357\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(prefix \u001b[39m+\u001b[39m _ffi\u001b[39m.\u001b[39mstring(err_str)\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mreplace\u001b[39m\u001b[39m'\u001b[39m))\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Error opening './UrbanSounds8K/fold1/101415-3-0-2.wav': System error."
          ]
        }
      ],
      "source": [
        "filename = \"./UrbanSounds8K/fold1/101415-3-0-2.wav\"\n",
        "waveform, sample_rate = rechannel(torchaudio.load(filename), 1)\n",
        "fig, axs = plt.subplots(2)\n",
        "axs[0].plot(waveform.t().numpy())\n",
        "show_spectrogram(waveform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7plnCUzxb0X"
      },
      "source": [
        "**Extended Audio**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "hGPQRvaPu5M5",
        "outputId": "44b74d4f-5844-4318-c19d-bcdb62148867"
      },
      "outputs": [],
      "source": [
        "waveform, sr = pad_trunc(torchaudio.load(filename), 4000)\n",
        "fig, axs = plt.subplots(2)\n",
        "axs[0].plot(waveform.t().numpy())\n",
        "fig.legend(['Original', 'Truncated'])\n",
        "show_spectrogram(waveform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDJnu38rWu9v"
      },
      "outputs": [],
      "source": [
        "spectrogram_tensor = torchaudio.transforms.Spectrogram()(waveform[0])\n",
        "# plt.imsave(f'test_air conditioner_spec.png', spectrogram_tensor[0].log2().numpy(), origin = 'lower', cmap='viridis')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "BRHwD2GXt5I-",
        "outputId": "b861dfb2-0fcb-4d18-f807-e5e258f6fbfb"
      },
      "outputs": [],
      "source": [
        "ipd.Audio(waveform.numpy(), rate=sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3dTkfPTe2zM"
      },
      "source": [
        "**Generate Spectrogram**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPmSPvC_fCnw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuz6GrNgpT4h"
      },
      "outputs": [],
      "source": [
        "class_map = ['air conditioner', 'car horn', 'children playing', 'dog bark', 'drilling', 'engine idling', 'gunshot', 'jackhammer', 'siren', 'street music']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIg6zff1e91w"
      },
      "outputs": [],
      "source": [
        "def load_audio_files(path, dataset):\n",
        "    walker = sorted(str(p) for p in Path(path).glob(f'*.wav'))\n",
        "    for i, file_path in enumerate(walker):\n",
        "        path, filename = os.path.split(file_path)\n",
        "        title, _ = os.path.splitext(filename)\n",
        "        fsID, classID, occurrenceID, sliceID = [int(n) for n in title.split('-')]\n",
        "        # Load audio\n",
        "        waveform, sample_rate = pad_trunc(resample(rechannel(torchaudio.load(file_path),1),24000),4000)\n",
        "        assert waveform.shape == torch.Size([1,96000]), f'Error: waveform shape is {waveform.shape}'\n",
        "        dataset.append([waveform[0], classID, title])\n",
        "        \n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnFf3KEwhOfD",
        "outputId": "a7fc6def-f1d8-43c3-aafc-ac884c31a1b4"
      },
      "outputs": [],
      "source": [
        "audio_dataset = []\n",
        "\n",
        "main_path = './UrbanSounds8K/'\n",
        "\n",
        "dir = [str(p) for p in Path(main_path).glob('fold*')]\n",
        "\n",
        "for path in dir:\n",
        "  print('Loading: '+ path)\n",
        "  audio_dataset = load_audio_files(path, audio_dataset)\n",
        "\n",
        "print(f\"Length of dataset: {len(audio_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFWwxexLmNF_"
      },
      "outputs": [],
      "source": [
        "audio_dataloader = torch.utils.data.DataLoader(audio_dataset, batch_size=1, shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCRDz5APacE5"
      },
      "outputs": [],
      "source": [
        "rm -rf ./UrbanSounds8K/spectrograms/ # remove all spectrograms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uByuBN-4b8UN"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def scale_minmax(X, min=0.0, max=1.0):\n",
        "    X_std = (X - X.min()) / (X.max() - X.min())\n",
        "    X_scaled = X_std * (max - min) + min\n",
        "    return X_scaled\n",
        "\n",
        "\n",
        "def create_spectrogram_images(trainloader):\n",
        "    length = len(trainloader)\n",
        "    spectrogram = torchaudio.transforms.Spectrogram(n_fft=512)\n",
        "    timeMask = torchaudio.transforms.TimeMasking(time_mask_param=80)\n",
        "    freqMask = torchaudio.transforms.FrequencyMasking(freq_mask_param=80)\n",
        "    num_TimeMask = 4\n",
        "    num_FreqMask = 4\n",
        "\n",
        "    for i, data in enumerate(trainloader):\n",
        "        waveform = data[0]\n",
        "        classID = data[1].item()\n",
        "        title = data[2][0]\n",
        "        directory = f'./UrbanSounds8K/spectrograms/{class_map[classID]}/'\n",
        "        #make directory\n",
        "        if(not os.path.isdir(directory)):\n",
        "            os.makedirs(directory, mode=0o777, exist_ok=True)\n",
        "        spectrogram_tensor = (spectrogram(waveform) + 1e-12).log2()\n",
        "        assert spectrogram_tensor.shape == torch.Size([1, 257, 376]), f\"Spectrogram size mismatch! {spectrogram_tensor.shape}\"\n",
        "        # plt.imshow(spectrogram_tensor[0])\n",
        "        # plt.show()\n",
        "\n",
        "        for a in range(num_TimeMask):\n",
        "            # create transformed waveforms\n",
        "            masked_spectrogram_tensor = timeMask(spectrogram_tensor)\n",
        "            # plt.imshow(masked_spectrogram_tensor[0])\n",
        "            # plt.show()\n",
        "            # skimage.io.imsave(f'{directory}{title}_timemasked{i}_spec.png', masked_spectrogram_tensor.flipud())\n",
        "            np.save(f'{directory}{title}-tm{a}_spec',\n",
        "                    masked_spectrogram_tensor.flipud())\n",
        "\n",
        "        for a in range(num_FreqMask):\n",
        "            masked_spectrogram_tensor = freqMask(spectrogram_tensor)\n",
        "            # plt.imshow(masked_spectrogram_tensor[0])\n",
        "            # plt.show()\n",
        "            # skimage.io.imsave(f'{directory}{title}_freqmasked{i}_spec.png', masked_spectrogram_tensor.flipud())\n",
        "            np.save(f'{directory}{title}-fm{a}_spec',\n",
        "                    masked_spectrogram_tensor.flipud())\n",
        "\n",
        "        # skimage.io.imsave(f'{directory}{title}_spec.png', spectrogram_tensor.flipud())\n",
        "        np.save(f'{directory}{title}-org_spec', spectrogram_tensor.flipud())\n",
        "        # break\n",
        "        if i % 1000 == 0:\n",
        "            print(f'{i}/{length}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaS8Zb04dGJA",
        "outputId": "d7467483-1273-4645-fc6c-5a046d3f808a"
      },
      "outputs": [],
      "source": [
        "create_spectrogram_images(audio_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "_DUePztj1BhY",
        "outputId": "71d27d64-7a01-43b5-950a-7b7c92d46d0a"
      },
      "outputs": [],
      "source": [
        "test_load = np.load('./UrbanSounds8K/spectrograms/air conditioner/162103-0-0-0-fm0_spec.npy')\n",
        "plt.imshow(test_load[0], cmap=\"gray\")\n",
        "print(test_load.min(), \" \", test_load.max())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OksgqWXLlGzx"
      },
      "source": [
        "**Actual Machine Learning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XHxisXgPOeN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import IPython.display as ipd\n",
        "from IPython.display import Audio\n",
        "import random\n",
        "from torchvision import datasets, transforms\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torchvision import datasets, models, transforms\n",
        "# from torchinfo import summary\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHCkJ1iPqSiu",
        "outputId": "e4d42133-d1b6-4ae9-82ab-71efab8aa770"
      },
      "outputs": [],
      "source": [
        "def npy_loader(path):\n",
        "    sample = torch.from_numpy(np.load(path))\n",
        "    return sample\n",
        "\n",
        "\n",
        "audio_dataset = datasets.DatasetFolder(root='./UrbanSounds8K/spectrograms/',\n",
        "                                       loader=npy_loader,\n",
        "                                       extensions=['.npy'])\n",
        "\n",
        "\n",
        "#split data to test and train\n",
        "#use 80% to train\n",
        "train_size = int(0.8 * len(audio_dataset))\n",
        "test_size = len(audio_dataset) - train_size\n",
        "audio_train_dataset, audio_test_dataset = torch.utils.data.random_split(audio_dataset, [train_size, test_size])\n",
        "\n",
        "print(\"Training size:\", len(audio_train_dataset))\n",
        "print(\"Testing size:\",len(audio_test_dataset))\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(audio_train_dataset,\n",
        "                                               batch_size=32,\n",
        "                                               num_workers=2,\n",
        "                                               shuffle=True)\n",
        "\n",
        "test_dataloader = torch.utils.data.DataLoader(audio_test_dataset,\n",
        "                                              batch_size=32,\n",
        "                                              num_workers=2,\n",
        "                                              shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3Hxk6wuqo0a"
      },
      "outputs": [],
      "source": [
        "# from collections import Counter\n",
        "# train_classes = [label for _, label in audio_train_dataset]\n",
        "# Counter(train_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDDNLI2nlfi4"
      },
      "source": [
        "Tensorboard (remove existing logs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MJQlAMOlfMn"
      },
      "outputs": [],
      "source": [
        "# rm -rf ./logsdir/ # remove all logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcug7HVWjSMF"
      },
      "source": [
        "Tensorboard (images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cI34T_QjR3P"
      },
      "outputs": [],
      "source": [
        "# writer=SummaryWriter('./content/logsdir')\n",
        "# first_batch = next(iter(train_dataloader))\n",
        "# img_grid = torchvision.utils.make_grid(first_batch[0])\n",
        "# writer.add_image('Spectrogram Images', img_grid)\n",
        "# writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eyYKk3h9QOHp",
        "outputId": "bd656937-2fe0-4dd3-e2d9-53d7d18650d0"
      },
      "outputs": [],
      "source": [
        "for i in range(10):\n",
        "    td = train_dataloader.dataset[i]\n",
        "    print(f'Image Class: {td[1]}')\n",
        "\n",
        "    plt.imshow(td[0][0].numpy(), cmap='inferno', origin='lower')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6i6wvLmc5F8R",
        "outputId": "be68547e-9b1c-4894-c012-5caae8ebc5b2"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from torch.nn import init\n",
        "\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18')\n",
        "model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "print(model.conv1)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Check that it is on Cuda\n",
        "next(model.parameters()).device\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVrAKQinQfWA"
      },
      "outputs": [],
      "source": [
        "# cost function used to determine best parameters\n",
        "cost = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# used to create optimal parameters\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Create the training function\n",
        "def train(dataloader, model, optimizer):\n",
        "    train_size = len(dataloader.dataset)\n",
        "    batch_size = len(dataloader)\n",
        "    train_loss, train_accuracy = 0, 0\n",
        "\n",
        "    model.train()\n",
        "    \n",
        "    for batch, (X, Y) in enumerate(dataloader):\n",
        "        X, Y = X.to(device), Y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(X)\n",
        "        batch_loss = cost(pred, Y)\n",
        "        batch_accuracy = (pred.argmax(1)==Y).type(torch.float).sum()\n",
        "        train_loss += batch_loss.item()\n",
        "        train_accuracy += batch_accuracy.item()\n",
        "        batch_loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch % 1000 == 0:\n",
        "            print(f\"Training batch {batch}/{batch_size} -> Loss: {batch_loss.item()}  Accuracy: {batch_accuracy.item()}%\")\n",
        "    train_loss /= train_size\n",
        "    train_accuracy /= train_size/100\n",
        "    return(train_loss, train_accuracy)\n",
        "\n",
        "\n",
        "# Create the validation/test function\n",
        "def test(dataloader, model):\n",
        "    test_size = len(dataloader.dataset)\n",
        "    batch_size = len(dataloader)\n",
        "    test_loss, test_accuracy = 0, 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch, (X, Y) in enumerate(dataloader):\n",
        "            X, Y = X.to(device), Y.to(device)\n",
        "            pred = model(X)\n",
        "            batch_loss = cost(pred, Y)\n",
        "            batch_accuracy = (pred.argmax(1)==Y).type(torch.float).sum()\n",
        "            test_loss += batch_loss.item()\n",
        "            test_accuracy += batch_accuracy.item()\n",
        "        if batch % 1000 == 0:\n",
        "            print(f\"Testing batch {batch}/{batch_size} -> Loss: {batch_loss.item()}  Accuracy: {batch_accuracy.item()}%\")\n",
        "\n",
        "    test_loss /= test_size\n",
        "    test_accuracy /= test_size/100\n",
        "    return(test_loss, test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aFppr-urj8t"
      },
      "source": [
        "Tensorboard model graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQEUozxSkwc9"
      },
      "outputs": [],
      "source": [
        "# writer.add_graph(model, first_batch[0].to(device))\n",
        "# writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nalCtoeHsRrv"
      },
      "outputs": [],
      "source": [
        "def tensorBoardLogging(train_loss, train_accuracy, test_loss, test_accuracy, epoch):\n",
        "    writer.add_scalar('1 Training/1 Model loss', train_loss, epoch)\n",
        "    writer.add_scalar('1 Training/2 Model accuracy', train_accuracy, epoch)\n",
        "    writer.add_scalar('2 Testing/1 Model loss', test_loss, epoch)\n",
        "    writer.add_scalar('2 Testing/2 Model accuracy', test_accuracy, epoch)\n",
        "    writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "obI3fgwAQhLP",
        "outputId": "8df092c6-e4c0-46f0-fcc8-fb2e7903291a"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f'Epoch {epoch+1}/{epochs}\\n-------------------------------')\n",
        "    start = time.time()\n",
        "    train_loss, train_accuracy = train(train_dataloader, model, optimizer)\n",
        "    test_loss, test_accuracy = test(test_dataloader, model)    \n",
        "    end = time.time()\n",
        "    print(f\"Epoch duration: {end - start} seconds\")\n",
        "    print(f'Training | Loss: {train_loss} Accuracy: {train_accuracy}%')\n",
        "    print(f'Testing  | Loss: {test_loss} Accuracy: {test_accuracy}% \\n')    \n",
        "    # tensorBoardLogging(train_loss, train_accuracy, test_loss, test_accuracy, epoch)\n",
        "\n",
        "print('Done!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ky3rlnX-1LT"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir /content/content/logsdir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8r1cXJnxF110"
      },
      "source": [
        "**Testing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIAP8Nvg88ag"
      },
      "source": [
        "**Debugging**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RumP79RPD4v6",
        "outputId": "158f193d-1fbe-4000-90b8-97bb285389bc"
      },
      "outputs": [],
      "source": [
        "# audio_dataset = []\n",
        "# sum = 0\n",
        "\n",
        "# main_path = './UrbanSounds8K/spectrograms/'\n",
        "\n",
        "# dir = [str(p) for p in Path(main_path).glob('*')]\n",
        "# for path in dir:\n",
        "#   print('Loading: '+ path)\n",
        "#   num = len([str(p) for p in Path(path).glob('*')])\n",
        "#   sum += num\n",
        "#   print(num)\n",
        "\n",
        "# sum"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.7 64-bit (microsoft store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "5e0ea5c72d63ec74a3e65528804f354af75f9a6d7105333063f2a8079cd404aa"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

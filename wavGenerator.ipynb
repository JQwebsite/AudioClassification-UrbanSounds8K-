{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "# files.upload()\n",
    "# ! pip install -q kaggle\n",
    "# ! mkdir ~/.kaggle\n",
    "# ! cp kaggle.json ~/.kaggle/\n",
    "# ! chmod 600 ~/.kaggle/kaggle.json\n",
    "# ! kaggle datasets download -d chrisfilo/urbansound8k\n",
    "# ! unzip urbansound8k.zip -d UrbanSounds8K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ipytest\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "import ipytest\n",
    "from pathlib import Path\n",
    "\n",
    "ipytest.autoconfig()\n",
    "\n",
    "class_map = [\n",
    "    'air conditioner', 'car horn', 'children playing', 'dog bark', 'drilling',\n",
    "    'engine idling', 'gunshot', 'jackhammer', 'siren', 'street music'\n",
    "]\n",
    "\n",
    "def pad_trunc(aud, max_ms):\n",
    "    sig, sr = aud\n",
    "    num_rows, sig_len = sig.shape\n",
    "    max_len = int(sr/1000 * max_ms)\n",
    "    if (sig_len > max_len):\n",
    "        # Truncate the signal to the given length\n",
    "        sig = sig[:,:max_len]\n",
    "    elif (sig_len < max_len):\n",
    "        # Length of padding to add at the beginning and end of the signal\n",
    "        pad_begin_len = random.randint(0, max_len - sig_len)\n",
    "        pad_end_len = max_len - sig_len - pad_begin_len\n",
    "        # Pad with 0s\n",
    "        pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
    "        pad_end = torch.zeros((num_rows, pad_end_len))\n",
    "        sig = torch.cat((pad_begin, sig, pad_end), 1)\n",
    "    return (sig, sr)\n",
    "\n",
    "def rechannel(aud, new_channel):\n",
    "    sig, sr = aud\n",
    "    if (sig.shape[0] == new_channel):\n",
    "        # Nothing to do\n",
    "        return aud\n",
    "    elif (new_channel == 1):\n",
    "        # Convert from stereo to mono by selecting only the first channel\n",
    "        resig = sig[:1, :]\n",
    "    else:\n",
    "        # Convert from mono to stereo by duplicating the first channel\n",
    "        resig = torch.cat([sig, sig])\n",
    "    return ((resig, sr))\n",
    "\n",
    "def resample(aud, newsr):\n",
    "    sig, sr = aud\n",
    "    if (sr == newsr):\n",
    "        # Nothing to do\n",
    "        return aud\n",
    "    num_channels = sig.shape[0]\n",
    "    # Resample first channel\n",
    "    resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1,:])\n",
    "    if (num_channels > 1):\n",
    "        # Resample the second channel and merge both channels\n",
    "        retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:,:])\n",
    "        resig = torch.cat([resig, retwo])\n",
    "    return ((resig, newsr))\n",
    "\n",
    "def show_spectrogram(audio):\n",
    "    spectrogram = torchaudio.transforms.Spectrogram()(audio)[0]\n",
    "    print(\"\\nShape of spectrogram: {}\".format(spectrogram.size()))\n",
    "    plt.imshow(spectrogram.log2().numpy(), cmap='viridis', origin='lower')\n",
    "    plt.show()\n",
    "\n",
    "def load_audio_files(path, dataset):\n",
    "    walker = [str(p) for p in Path(path).glob(f'*.wav')]\n",
    "    for i, file_path in enumerate(walker):\n",
    "        path, filename = os.path.split(file_path)\n",
    "        title, _ = os.path.splitext(filename)\n",
    "        fsID, classID, occurrenceID, sliceID = [\n",
    "            int(n) for n in title.split('-')\n",
    "        ]\n",
    "        # Load audio\n",
    "        waveform, sample_rate = pad_trunc(\n",
    "            resample(rechannel(torchaudio.load(file_path), 1), 44100), 4000)\n",
    "        assert waveform.shape == torch.Size(\n",
    "            [1, 176400]), f'Error: waveform shape is {waveform.shape}'\n",
    "        dataset.append([waveform[0], classID, title])\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def create_spectrogram_images(trainloader, train=True):\n",
    "    length = len(trainloader)\n",
    "    spectrogram = torchaudio.transforms.Spectrogram()\n",
    "    timeMask = torchaudio.transforms.TimeMasking(time_mask_param=80)\n",
    "    freqMask = torchaudio.transforms.FrequencyMasking(freq_mask_param=80)\n",
    "    num_TimeMask = 4\n",
    "    num_FreqMask = 4\n",
    "    num_FreqTimeMask = 4\n",
    "\n",
    "    for i, data in enumerate(trainloader):\n",
    "        waveform = data[0]\n",
    "        classID = data[1].item()\n",
    "        title = data[2][0]\n",
    "        if train:\n",
    "            directory = f'./UrbanSounds8K/spectrograms/train/{class_map[classID]}/'\n",
    "        else:\n",
    "            directory = f'./UrbanSounds8K/spectrograms/val/{class_map[classID]}/'\n",
    "        if (not os.path.isdir(directory)):\n",
    "            os.makedirs(directory, mode=0o777, exist_ok=True)\n",
    "\n",
    "        spectrogram_tensor = (spectrogram(waveform) + 1e-12).log2()\n",
    "\n",
    "        assert spectrogram_tensor.shape == torch.Size(\n",
    "            [1, 201,\n",
    "             883]), f\"Spectrogram size mismatch! {spectrogram_tensor.shape}\"\n",
    "\n",
    "        if train:\n",
    "            # create transformed waveforms\n",
    "            for a in range(num_TimeMask):\n",
    "                masked_spectrogram_tensor = timeMask(spectrogram_tensor)\n",
    "                np.save(f'{directory}{title}-tm{a}_spec',\n",
    "                        masked_spectrogram_tensor.flipud())\n",
    "\n",
    "            for a in range(num_FreqMask):\n",
    "                masked_spectrogram_tensor = freqMask(spectrogram_tensor)\n",
    "                np.save(f'{directory}{title}-fm{a}_spec',\n",
    "                        masked_spectrogram_tensor.flipud())\n",
    "\n",
    "            for a in range(num_FreqTimeMask):\n",
    "                masked_spectrogram_tensor = freqMask(\n",
    "                    timeMask(spectrogram_tensor))\n",
    "                np.save(f'{directory}{title}-ftm{a}_spec',\n",
    "                        masked_spectrogram_tensor.flipud())\n",
    "\n",
    "        np.save(f'{directory}{title}-org_spec', spectrogram_tensor.flipud())\n",
    "        # break\n",
    "        if i % 1000 == 0:\n",
    "            print(f'{i}/{length}')\n",
    "\n",
    "    return (length - 1) * (num_TimeMask + num_FreqMask + num_FreqTimeMask +\n",
    "                           1) if train else (length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "\n",
    "def test_load():\n",
    "    walker = sorted(str(p) for p in Path(\"./test/\").glob(f'*.wav'))\n",
    "    for i, file_path in enumerate(walker):\n",
    "        assert torchaudio.load(file_path) \n",
    "\n",
    "def test_pad_trunc():\n",
    "    target_length = 4\n",
    "    assert len(pad_trunc(torchaudio.load('./test/1_44100_0830.wav'), target_length * 1000)[0][0]) == 44100*target_length\n",
    "    assert len(pad_trunc(torchaudio.load('./test/2_44100_2250.wav'), target_length * 1000)[0][0]) == 44100*target_length\n",
    "    assert len(pad_trunc(torchaudio.load('./test/2_44100_4000.wav'), target_length * 1000)[0][0]) == 44100*target_length\n",
    "\n",
    "def test_rechannel():\n",
    "    target_channel = 1\n",
    "    assert rechannel(torchaudio.load('./test/1_44100_0830.wav'), target_channel)[0].shape[0] == target_channel\n",
    "    assert rechannel(torchaudio.load('./test/2_44100_4000.wav'), target_channel)[0].shape[0] == target_channel\n",
    "\n",
    "def test_resample():\n",
    "    target_sr = 44100\n",
    "    assert len(resample(torchaudio.load('./test/1_11025_4000.wav'), target_sr)[0][0]) == 4*target_sr\n",
    "    assert len(resample(torchaudio.load('./test/1_44100_0830.wav'), target_sr)[0][0]) == 0.830*target_sr\n",
    "    assert len(resample(torchaudio.load('./test/1_96000_0310.wav'), target_sr)[0][0]) == 0.310*target_sr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dataset = []\n",
    "\n",
    "main_path = './UrbanSounds8K/'\n",
    "\n",
    "dir = [str(p) for p in Path(main_path).glob('fold*')][0:-1]\n",
    "test_val = dir[0:2]\n",
    "\n",
    "for path in test_val:\n",
    "    print(\"Loading \", path)\n",
    "    audio_dataset = load_audio_files(path, audio_dataset)\n",
    "\n",
    "print(f\"Length of dataset: {len(audio_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(audio_dataset))\n",
    "val_size = len(audio_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    audio_dataset, [train_size, val_size])\n",
    "\n",
    "audio_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=1,\n",
    "                                               shuffle=False,\n",
    "                                               num_workers=0)\n",
    "\n",
    "num_train = create_spectrogram_images(audio_dataloader, True)\n",
    "\n",
    "audio_dataloader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                               batch_size=1,\n",
    "                                               shuffle=False,\n",
    "                                               num_workers=0)\n",
    "\n",
    "num_val = create_spectrogram_images(audio_dataloader, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "\n",
    "%%ipytest\n",
    "\n",
    "def test_spectrogram_generation_train():\n",
    "    assert len([\n",
    "        str(p) for p in Path('./UrbanSounds8K/spectrograms/train').glob('*')\n",
    "    ]) == len(class_map)\n",
    "\n",
    "    sum = 0\n",
    "    dir = [str(p) for p in Path('./UrbanSounds8K/spectrograms/train').glob('*')]\n",
    "    for path in dir:\n",
    "        num = len([str(p) for p in Path(path).glob('*')])\n",
    "        sum += num\n",
    "    assert sum == num_train\n",
    "\n",
    "def test_spectrogram_generation_val():\n",
    "    assert len([\n",
    "        str(p) for p in Path('./UrbanSounds8K/spectrograms/val').glob('*')\n",
    "    ]) == len(class_map)\n",
    "\n",
    "    sum = 0\n",
    "    dir = [str(p) for p in Path('./UrbanSounds8K/spectrograms/val').glob('*')]\n",
    "    for path in dir:\n",
    "        num = len([str(p) for p in Path(path).glob('*')])\n",
    "        sum += num\n",
    "\n",
    "    assert sum == num_val\n",
    "\n",
    "def test_spectrogram_generation_test():\n",
    "    sum = 0\n",
    "    dir = [str(p) for p in Path('./UrbanSounds8K/spectrograms/test').glob('*')]\n",
    "    for path in dir:\n",
    "        num = len([str(p) for p in Path(path).glob('*')])\n",
    "        sum += num\n",
    "\n",
    "    assert sum == num_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_load = np.load(\n",
    "    './UrbanSounds8K/spectrograms/train/gunshot/122690-6-0-0-fm0_spec.npy')\n",
    "plt.imshow(test_load[0], origin=\"lower\")\n",
    "plt.colorbar()\n",
    "print(test_load.min(), \" \", test_load.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = \"./UrbanSounds8K/fold1/102305-6-0-0.wav\"\n",
    "# waveform, sample_rate = rechannel(torchaudio.load(filename), 1)\n",
    "# fig, axs = plt.subplots(2)\n",
    "# axs[0].plot(waveform.t().numpy())\n",
    "# show_spectrogram(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# waveform, sr = pad_trunc(rechannel(torchaudio.load(filename), 1), 4000)\n",
    "# fig, axs = plt.subplots(2)\n",
    "# axs[0].plot(waveform.t().numpy())\n",
    "# show_spectrogram(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_dataset = []\n",
    "# sum = 0\n",
    "\n",
    "# main_path = './UrbanSounds8K/spectrograms/'\n",
    "\n",
    "# dir = [str(p) for p in Path(main_path).glob('*')]\n",
    "# for path in dir:\n",
    "#   print('Loading: '+ path)\n",
    "#   num = len([str(p) for p in Path(path).glob('*')])\n",
    "#   sum += num\n",
    "#   print(num)\n",
    "\n",
    "# sum"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5e0ea5c72d63ec74a3e65528804f354af75f9a6d7105333063f2a8079cd404aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NcmtmsaxbwTy"
   },
   "source": [
    "**Download dataset from Kaggle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gmWyqTs98kyQ",
    "outputId": "530bcedc-551f-4ee9-f4e2-018ce6093e41"
   },
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "# files.upload()\n",
    "# ! pip install -q kaggle\n",
    "# ! mkdir ~/.kaggle\n",
    "# ! cp kaggle.json ~/.kaggle/\n",
    "# ! chmod 600 ~/.kaggle/kaggle.json\n",
    "# ! kaggle datasets download -d chrisfilo/urbansound8k\n",
    "# ! unzip urbansound8k.zip -d UrbanSounds8K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmVjVvU3cYGZ"
   },
   "source": [
    "**Inspect the audio files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WP4iapU3B5dH"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torchaudio\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "huXMRna2t-YC"
   },
   "outputs": [],
   "source": [
    "def show_spectrogram(audio):\n",
    "  # only shows first channel\n",
    "    spectrogram = torchaudio.transforms.Spectrogram()(audio)[0]\n",
    "    print(\"\\nShape of spectrogram: {}\".format(spectrogram.size()))\n",
    "    plt.imshow(spectrogram.log2().numpy(), cmap='viridis', origin='lower')\n",
    "    plt.show()\n",
    "\n",
    "def pad_trunc(aud, max_ms):\n",
    "    sig, sr = aud\n",
    "    num_rows, sig_len = sig.shape\n",
    "    max_len = sr//1000 * max_ms\n",
    "\n",
    "    if (sig_len == max_len):\n",
    "      return\n",
    "\n",
    "    elif (sig_len > max_len):\n",
    "      # Truncate the signal to the given length\n",
    "      sig = sig[:,:max_len]\n",
    "\n",
    "    else:\n",
    "      # Length of padding to add at the beginning and end of the signal\n",
    "      pad_begin_len = random.randint(0, max_len - sig_len)\n",
    "      pad_end_len = max_len - sig_len - pad_begin_len\n",
    "\n",
    "      # Pad with 0s\n",
    "      pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
    "      pad_end = torch.zeros((num_rows, pad_end_len))\n",
    "\n",
    "      sig = torch.cat((pad_begin, sig, pad_end), 1)\n",
    "      \n",
    "    return (sig, sr)\n",
    "\n",
    "def rechannel(aud, new_channel):\n",
    "    sig, sr = aud\n",
    "\n",
    "    if (sig.shape[0] == new_channel):\n",
    "      # Nothing to do\n",
    "      return aud\n",
    "\n",
    "    elif (new_channel == 1):\n",
    "      # Convert from stereo to mono by selecting only the first channel\n",
    "      resig = sig[:1, :]\n",
    "    else:\n",
    "      # Convert from mono to stereo by duplicating the first channel\n",
    "      resig = torch.cat([sig, sig])\n",
    "\n",
    "    return ((resig, sr))\n",
    "\n",
    "def resample(aud, newsr):\n",
    "    sig, sr = aud\n",
    "\n",
    "    if (sr == newsr):\n",
    "      # Nothing to do\n",
    "      return aud\n",
    "\n",
    "    num_channels = sig.shape[0]\n",
    "    # Resample first channel\n",
    "    resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1,:])\n",
    "    if (num_channels > 1):\n",
    "      # Resample the second channel and merge both channels\n",
    "      retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:,:])\n",
    "      resig = torch.cat([resig, retwo])\n",
    "\n",
    "    return ((resig, newsr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1W6OFH5xYDJ"
   },
   "source": [
    "**Original Audio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "pFJgdpckbhB5",
    "outputId": "c4236b0e-ef4d-4c76-f349-e62df95ec4dd"
   },
   "outputs": [],
   "source": [
    "filename = \"./UrbanSounds8K/fold1/102305-6-0-0.wav\"\n",
    "waveform, sample_rate = rechannel(torchaudio.load(filename), 1)\n",
    "fig, axs = plt.subplots(2)\n",
    "axs[0].plot(waveform.t().numpy())\n",
    "show_spectrogram(waveform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7plnCUzxb0X"
   },
   "source": [
    "**Extended Audio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "id": "hGPQRvaPu5M5",
    "outputId": "44b74d4f-5844-4318-c19d-bcdb62148867"
   },
   "outputs": [],
   "source": [
    "waveform, sr = pad_trunc(rechannel(torchaudio.load(filename),1), 4000)\n",
    "fig, axs = plt.subplots(2)\n",
    "axs[0].plot(waveform.t().numpy())\n",
    "show_spectrogram(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "BRHwD2GXt5I-",
    "outputId": "b861dfb2-0fcb-4d18-f807-e5e258f6fbfb"
   },
   "outputs": [],
   "source": [
    "ipd.Audio(waveform.numpy(), rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3dTkfPTe2zM"
   },
   "source": [
    "**Generate Spectrogram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SPmSPvC_fCnw"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "# import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vuz6GrNgpT4h"
   },
   "outputs": [],
   "source": [
    "class_map = ['air conditioner', 'car horn', 'children playing', 'dog bark', 'drilling', 'engine idling', 'gunshot', 'jackhammer', 'siren', 'street music']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CIg6zff1e91w"
   },
   "outputs": [],
   "source": [
    "def load_audio_files(path, dataset):\n",
    "    walker = sorted(str(p) for p in Path(path).glob(f'*.wav'))\n",
    "    for i, file_path in enumerate(walker):\n",
    "        path, filename = os.path.split(file_path)\n",
    "        title, _ = os.path.splitext(filename)\n",
    "        fsID, classID, occurrenceID, sliceID = [int(n) for n in title.split('-')]\n",
    "        # Load audio\n",
    "        waveform, sample_rate = pad_trunc(resample(rechannel(torchaudio.load(file_path),1),44100),4000)\n",
    "        assert waveform.shape == torch.Size([1,176000]), f'Error: waveform shape is {waveform.shape}'\n",
    "        dataset.append([waveform[0], classID, title])\n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fnFf3KEwhOfD",
    "outputId": "a7fc6def-f1d8-43c3-aafc-ac884c31a1b4"
   },
   "outputs": [],
   "source": [
    "audio_dataset = []\n",
    "\n",
    "main_path = './UrbanSounds8K/'\n",
    "\n",
    "dir = [str(p) for p in Path(main_path).glob('fold*')][0:1]\n",
    "\n",
    "for path in dir:\n",
    "  print(\"Loading \", path)\n",
    "  audio_dataset = load_audio_files(path, audio_dataset)\n",
    "\n",
    "print(f\"Length of dataset: {len(audio_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SCRDz5APacE5"
   },
   "outputs": [],
   "source": [
    "# rm -rf ./UrbanSounds8K/spectrograms/ # remove all spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uByuBN-4b8UN"
   },
   "outputs": [],
   "source": [
    "def scale_minmax(X, min=0.0, max=1.0):\n",
    "    X_std = (X - X.min()) / (X.max() - X.min())\n",
    "    X_scaled = X_std * (max - min) + min\n",
    "    return X_scaled\n",
    "\n",
    "def create_spectrogram_images(trainloader, train=True):\n",
    "    length = len(trainloader)\n",
    "    spectrogram = torchaudio.transforms.Spectrogram()\n",
    "    timeMask = torchaudio.transforms.TimeMasking(time_mask_param=80, )\n",
    "    freqMask = torchaudio.transforms.FrequencyMasking(freq_mask_param=80)\n",
    "    num_TimeMask = 4\n",
    "    num_FreqMask = 4\n",
    "    num_FreqTimeMask = 4\n",
    "\n",
    "    for i, data in enumerate(trainloader):\n",
    "        waveform = data[0]\n",
    "        classID = data[1].item()\n",
    "        title = data[2][0]\n",
    "        if train:\n",
    "            directory = f'./UrbanSounds8K/spectrograms/train/{class_map[classID]}/'\n",
    "        else:\n",
    "            directory = f'./UrbanSounds8K/spectrograms/test/{class_map[classID]}/'\n",
    "        if(not os.path.isdir(directory)):\n",
    "            os.makedirs(directory, mode=0o777, exist_ok=True)\n",
    "\n",
    "        spectrogram_tensor = (spectrogram(waveform) + 1e-12).log2()\n",
    "\n",
    "        assert spectrogram_tensor.shape == torch.Size([1, 201, 881]), f\"Spectrogram size mismatch! {spectrogram_tensor.shape}\"\n",
    "        if train:\n",
    "            for a in range(num_TimeMask):\n",
    "                # create transformed waveforms\n",
    "                masked_spectrogram_tensor = timeMask(spectrogram_tensor)\n",
    "                np.save(f'{directory}{title}-tm{a}_spec',\n",
    "                        masked_spectrogram_tensor.flipud())\n",
    "\n",
    "            for a in range(num_FreqMask):\n",
    "                masked_spectrogram_tensor = freqMask(spectrogram_tensor)\n",
    "                np.save(f'{directory}{title}-fm{a}_spec',\n",
    "                        masked_spectrogram_tensor.flipud())\n",
    "\n",
    "            for a in range(num_FreqTimeMask):\n",
    "                masked_spectrogram_tensor = freqMask(timeMask(spectrogram_tensor))\n",
    "                np.save(f'{directory}{title}-ftm{a}_spec',\n",
    "                        masked_spectrogram_tensor.flipud())\n",
    "        \n",
    "        np.save(f'{directory}{title}-org_spec', spectrogram_tensor.flipud())\n",
    "        # break\n",
    "        if i % 1000 == 0:\n",
    "            print(f'{i}/{length}')\n",
    "\n",
    "\n",
    "\n",
    "train_size = int(0.8 * len(audio_dataset))\n",
    "test_size = len(audio_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    audio_dataset, [train_size, test_size])\n",
    "\n",
    "audio_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=1,\n",
    "                                               shuffle=False,\n",
    "                                               num_workers=0)\n",
    "\n",
    "create_spectrogram_images(audio_dataloader, True)\n",
    "\n",
    "audio_dataloader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                               batch_size=1,\n",
    "                                               shuffle=False,\n",
    "                                               num_workers=0)\n",
    "\n",
    "create_spectrogram_images(audio_dataloader, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "_DUePztj1BhY",
    "outputId": "71d27d64-7a01-43b5-950a-7b7c92d46d0a"
   },
   "outputs": [],
   "source": [
    "test_load = np.load(\n",
    "        './UrbanSounds8K/spectrograms/train/gunshot/122690-6-0-0-fm0_spec.npy')\n",
    "plt.imshow(test_load[0], origin=\"lower\")\n",
    "plt.colorbar()\n",
    "print(test_load.min(), \" \", test_load.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OksgqWXLlGzx"
   },
   "source": [
    "**Actual Machine Learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-XHxisXgPOeN"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import IPython.display as ipd\n",
    "from IPython.display import Audio\n",
    "import random\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torchvision import datasets, models, transforms\n",
    "# from torchinfo import summary\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VHCkJ1iPqSiu",
    "outputId": "e4d42133-d1b6-4ae9-82ab-71efab8aa770"
   },
   "outputs": [],
   "source": [
    "def npy_loader(path):\n",
    "    sample = torch.from_numpy(np.load(path))\n",
    "    return sample\n",
    "\n",
    "\n",
    "audio_train_dataset = datasets.DatasetFolder(root='./UrbanSounds8K/spectrograms/train/',\n",
    "                                       loader=npy_loader,\n",
    "                                       extensions=['.npy'])\n",
    "\n",
    "audio_test_dataset = datasets.DatasetFolder(root='./UrbanSounds8K/spectrograms/test/',\n",
    "                                       loader=npy_loader,\n",
    "                                       extensions=['.npy'])\n",
    "\n",
    "print(\"Training size:\", len(audio_train_dataset))\n",
    "print(\"Testing size:\",len(audio_test_dataset))\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(audio_train_dataset,\n",
    "                                               batch_size=64,\n",
    "                                               num_workers=8,\n",
    "                                               shuffle=True)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(audio_test_dataset,\n",
    "                                              batch_size=64,\n",
    "                                              num_workers=8,\n",
    "                                              shuffle=True,\n",
    "                                             )\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18')\n",
    "model.conv1 = nn.Conv2d(1,\n",
    "                        64,\n",
    "                        kernel_size=(7, 7),\n",
    "                        stride=(2, 2),\n",
    "                        padding=(3, 3),\n",
    "                        bias=False)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Check that it is on Cuda\n",
    "next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-MJQlAMOlfMn"
   },
   "outputs": [],
   "source": [
    "# rm -rf ./logsdir/ # remove all logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aVrAKQinQfWA"
   },
   "outputs": [],
   "source": [
    "# cost function used to determine best parameters\n",
    "cost = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# used to create optimal parameters\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create the training function\n",
    "def train(dataloader, model, optimizer):\n",
    "    train_size = len(dataloader.dataset)\n",
    "    batch_size = len(next(iter(dataloader))[1])\n",
    "    total_batch = len(dataloader)\n",
    "    train_loss, train_accuracy = 0, 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch, (X, Y) in enumerate(dataloader):\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X)\n",
    "        batch_loss = cost(pred, Y)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_accuracy = (pred.argmax(1)==Y).type(torch.float).sum()\n",
    "        train_loss += batch_loss.item()\n",
    "        train_accuracy += batch_accuracy.item()\n",
    "        if batch % 100 == 0:\n",
    "            print(\n",
    "                f\"Training batch {batch}/{total_batch} -> Loss: {batch_loss.item()}  Accuracy: {batch_accuracy.item()/batch_size*100}%\"\n",
    "            )\n",
    "    train_loss /= train_size\n",
    "    train_accuracy /= train_size/100\n",
    "    return(train_loss, train_accuracy)\n",
    "\n",
    "\n",
    "# Create the validation/test function\n",
    "def test(dataloader, model):\n",
    "    test_size = len(dataloader.dataset)\n",
    "    total_batch = len(dataloader)\n",
    "    test_loss, test_accuracy = 0, 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, Y) in enumerate(dataloader):\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            pred = model(X)\n",
    "            batch_loss = cost(pred, Y)\n",
    "            batch_accuracy = (pred.argmax(1)==Y).type(torch.float).sum()\n",
    "            test_loss += batch_loss.item()\n",
    "            test_accuracy += batch_accuracy.item()\n",
    "        if batch % 10 == 0:\n",
    "            print(\n",
    "                f\"Testing batch {batch}/{total_batch} -> Loss: {batch_loss.item()}  Accuracy: {batch_accuracy.item()/batch_size*100}%\"\n",
    "            )\n",
    "\n",
    "    test_loss /= test_size\n",
    "    test_accuracy /= test_size/100\n",
    "    return(test_loss, test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4aFppr-urj8t"
   },
   "source": [
    "Tensorboard model graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "bQEUozxSkwc9"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "title = datetime.now().strftime(\"%Y-%m-%d,%H-%M-%S\")\n",
    "\n",
    "title=\"freqTimeFulldataset\"\n",
    "\n",
    "first_batch = next(iter(train_dataloader))\n",
    "writer = SummaryWriter(f'./logs/{title}')\n",
    "writer.add_graph(model, first_batch[0].to(device))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nalCtoeHsRrv"
   },
   "outputs": [],
   "source": [
    "def tensorBoardLogging(train_loss, train_accuracy, test_loss, test_accuracy, epoch):\n",
    "    writer.add_scalar('1 Training/1 Model loss', train_loss, epoch)\n",
    "    writer.add_scalar('1 Training/2 Model accuracy', train_accuracy, epoch)\n",
    "    writer.add_scalar('2 Testing/1 Model loss', test_loss, epoch)\n",
    "    writer.add_scalar('2 Testing/2 Model accuracy', test_accuracy, epoch)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 432
    },
    "id": "obI3fgwAQhLP",
    "outputId": "8df092c6-e4c0-46f0-fcc8-fb2e7903291a"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "epochs = 20\n",
    "testa_history = []\n",
    "traina_history = []\n",
    "testl_history = []\n",
    "trainl_history = []\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch+1}/{epochs}\\n-------------------------------')\n",
    "    start = time.time()\n",
    "    train_loss, train_accuracy = train(train_dataloader, model, optimizer)\n",
    "    test_loss, test_accuracy = test(test_dataloader, model)    \n",
    "    end = time.time()\n",
    "    print(f\"\\nEpoch duration: {end - start} seconds\")\n",
    "    print(f'Training | Loss: {train_loss} Accuracy: {train_accuracy}%')\n",
    "    print(f'Testing  | Loss: {test_loss} Accuracy: {test_accuracy}% \\n') \n",
    "    traina_history.append(train_accuracy)\n",
    "    trainl_history.append(train_loss)\n",
    "    testa_history.append(test_accuracy)\n",
    "    testl_history.append(test_loss)\n",
    "    print(train_accuracy)\n",
    "    # tensorBoardLogging(train_loss, train_accuracy, test_loss, test_accuracy, epoch)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIAP8Nvg88ag"
   },
   "source": [
    "**Debugging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RumP79RPD4v6",
    "outputId": "158f193d-1fbe-4000-90b8-97bb285389bc"
   },
   "outputs": [],
   "source": [
    "# audio_dataset = []\n",
    "# sum = 0\n",
    "\n",
    "# main_path = './UrbanSounds8K/spectrograms/'\n",
    "\n",
    "# dir = [str(p) for p in Path(main_path).glob('*')]\n",
    "# for path in dir:\n",
    "#   print('Loading: '+ path)\n",
    "#   num = len([str(p) for p in Path(path).glob('*')])\n",
    "#   sum += num\n",
    "#   print(num)\n",
    "\n",
    "# sum"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "9e87201990ab3ef9f8e8015a432b605669f23d2887f787d8acc74a222f9112d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "OksgqWXLlGzx"
            },
            "source": [
                "**Actual Machine Learning**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {
                "id": "-XHxisXgPOeN"
            },
            "outputs": [],
            "source": [
                "import os\n",
                "import IPython.display as ipd\n",
                "from IPython.display import Audio\n",
                "import random\n",
                "from torchvision import datasets, transforms\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "import numpy as np\n",
                "from torchvision import datasets, models, transforms\n",
                "# from torchinfo import summary\n",
                "import matplotlib.pyplot as plt\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from pathlib import Path\n",
                "import torchvision\n",
                "from torch.utils.tensorboard import SummaryWriter"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 153,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "VHCkJ1iPqSiu",
                "outputId": "e4d42133-d1b6-4ae9-82ab-71efab8aa770"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training size: 18304\n",
                        "Validation size: 353\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Using cache found in C:\\Users\\JianQuan/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "device(type='cuda', index=0)"
                        ]
                    },
                    "execution_count": 153,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "def npy_loader(path):\n",
                "    sample = torch.from_numpy(np.load(path))\n",
                "    return sample\n",
                "\n",
                "\n",
                "audio_train_dataset = datasets.DatasetFolder(root='./UrbanSounds8K/spectrograms/train/',\n",
                "                                       loader=npy_loader,\n",
                "                                       extensions=['.npy'])\n",
                "\n",
                "audio_val_dataset = datasets.DatasetFolder(root='./UrbanSounds8K/spectrograms/val/',\n",
                "                                       loader=npy_loader,\n",
                "                                       extensions=['.npy'])\n",
                "\n",
                "print(\"Training size:\", len(audio_train_dataset))\n",
                "print(\"Validation size:\",len(audio_val_dataset))\n",
                "\n",
                "train_dataloader = torch.utils.data.DataLoader(audio_train_dataset,\n",
                "                                               batch_size=64,\n",
                "                                               num_workers=0,\n",
                "                                               shuffle=True)\n",
                "\n",
                "val_dataloader = torch.utils.data.DataLoader(audio_val_dataset,\n",
                "                                              batch_size=64,\n",
                "                                              num_workers=0,\n",
                "                                              shuffle=True,\n",
                "                                             )\n",
                "\n",
                "import torch.nn.functional as F\n",
                "from torch.nn import init\n",
                "\n",
                "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18')\n",
                "model.conv1 = nn.Conv2d(1,\n",
                "                        64,\n",
                "                        kernel_size=(7, 7),\n",
                "                        stride=(2, 2),\n",
                "                        padding=(3, 3),\n",
                "                        bias=False)\n",
                "model.fc = nn.Linear(in_features=512, out_features=10, bias=True)\n",
                "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
                "model = model.to(device)\n",
                "\n",
                "# Check that it is on Cuda\n",
                "next(model.parameters()).device"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "id": "-MJQlAMOlfMn"
            },
            "outputs": [],
            "source": [
                "# rm -rf ./logsdir/ # remove all logs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 158,
            "metadata": {
                "id": "aVrAKQinQfWA"
            },
            "outputs": [],
            "source": [
                "# cost function used to determine best parameters\n",
                "cost = torch.nn.CrossEntropyLoss()\n",
                "\n",
                "# used to create optimal parameters\n",
                "learning_rate = 0.001\n",
                "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
                "\n",
                "# Create the training function\n",
                "def train(dataloader, model, optimizer):\n",
                "    train_size = len(dataloader.dataset)\n",
                "    batch_size = len(next(iter(dataloader))[1])\n",
                "    total_batch = len(dataloader)\n",
                "    train_loss, train_accuracy = 0, 0\n",
                "\n",
                "    model.train()\n",
                "\n",
                "    for batch, (X, Y) in enumerate(dataloader):\n",
                "        X, Y = X.to(device), Y.to(device)\n",
                "        optimizer.zero_grad()\n",
                "        pred = model(X)\n",
                "        batch_loss = cost(pred, Y)\n",
                "        batch_loss.backward()\n",
                "        optimizer.step()\n",
                "        batch_accuracy = (pred.argmax(1)==Y).type(torch.float).sum()\n",
                "        train_loss += batch_loss.item()\n",
                "        train_accuracy += batch_accuracy.item()\n",
                "        if batch % 100 == 0:\n",
                "            print(\n",
                "                f\"Training batch {batch}/{total_batch} -> Loss: {batch_loss.item()}  Accuracy: {batch_accuracy.item()/batch_size*100}%\"\n",
                "            )\n",
                "    train_loss /= train_size\n",
                "    train_accuracy /= train_size/100\n",
                "    return(train_loss, train_accuracy)\n",
                "\n",
                "\n",
                "# Create the validation function\n",
                "def val(dataloader, model):\n",
                "    val_size = len(dataloader.dataset)\n",
                "    total_batch = len(dataloader)\n",
                "    val_loss, val_accuracy = 0, 0\n",
                "\n",
                "    model.eval()\n",
                "\n",
                "    with torch.no_grad():\n",
                "        for batch, (X, Y) in enumerate(dataloader):\n",
                "            X, Y = X.to(device), Y.to(device)\n",
                "            pred = model(X)\n",
                "            batch_loss = cost(pred, Y)\n",
                "            batch_accuracy = (pred.argmax(1)==Y).type(torch.float).sum()\n",
                "            val_loss += batch_loss.item()\n",
                "            val_accuracy += batch_accuracy.item()\n",
                "        if batch % 10 == 0:\n",
                "            print(\n",
                "                f\"Validation batch {batch}/{total_batch} -> Loss: {batch_loss.item()}  Accuracy: {batch_accuracy.item()/batch_size*100}%\"\n",
                "            )\n",
                "\n",
                "    val_loss /= val_size\n",
                "    val_accuracy /= val_size/100\n",
                "    return(val_loss, val_accuracy)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "4aFppr-urj8t"
            },
            "source": [
                "Tensorboard model graph"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 148,
            "metadata": {},
            "outputs": [],
            "source": [
                "first_batch = next(iter(train_dataloader))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 156,
            "metadata": {
                "id": "bQEUozxSkwc9"
            },
            "outputs": [],
            "source": [
                "import time\n",
                "from datetime import datetime\n",
                "\n",
                "title = datetime.now().strftime(\"%Y-%m-%d,%H-%M-%S\")\n",
                "# title=\"freqTimeFulldataset10out\"\n",
                "\n",
                "writer = SummaryWriter(f'./logs/{title}')\n",
                "writer.add_graph(model, first_batch[0].to(device))\n",
                "writer.close()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {
                "id": "nalCtoeHsRrv"
            },
            "outputs": [],
            "source": [
                "def tensorBoardLogging(train_loss, train_accuracy, val_loss, val_accuracy, epoch):\n",
                "    writer.add_scalar('1 Training/1 Model loss', train_loss, epoch)\n",
                "    writer.add_scalar('1 Training/2 Model accuracy', train_accuracy, epoch)\n",
                "    writer.add_scalar('2 Validate/1 Model loss', val_loss, epoch)\n",
                "    writer.add_scalar('2 Validate/2 Model accuracy', val_accuracy, epoch)\n",
                "    writer.close()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 159,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 432
                },
                "id": "obI3fgwAQhLP",
                "outputId": "8df092c6-e4c0-46f0-fcc8-fb2e7903291a"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 1/20\n",
                        "-------------------------------\n",
                        "Training batch 0/286 -> Loss: 2.346491813659668  Accuracy: 10.9375%\n"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn [159], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 7\u001b[0m train_loss, train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m val_loss, val_accuracy \u001b[38;5;241m=\u001b[39m val(val_dataloader, model)\n\u001b[0;32m      9\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
                        "Cell \u001b[1;32mIn [158], line 25\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, optimizer)\u001b[0m\n\u001b[0;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     24\u001b[0m batch_accuracy \u001b[38;5;241m=\u001b[39m (pred\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m==\u001b[39mY)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m---> 25\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mbatch_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m train_accuracy \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_accuracy\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "import time\n",
                "\n",
                "epochs = 20\n",
                "for epoch in range(epochs):\n",
                "    print(f'Epoch {epoch+1}/{epochs}\\n-------------------------------')\n",
                "    start = time.time()\n",
                "    train_loss, train_accuracy = train(train_dataloader, model, optimizer)\n",
                "    val_loss, val_accuracy = val(val_dataloader, model)\n",
                "    end = time.time()\n",
                "    print(f\"\\nEpoch duration: {end - start} seconds\")\n",
                "    print(f'Training | Loss: {train_loss} Accuracy: {train_accuracy}%')\n",
                "    print(f'Validating  | Loss: {val_loss} Accuracy: {val_accuracy}% \\n')\n",
                "    tensorBoardLogging(train_loss, train_accuracy, val_loss, val_accuracy, epoch)\n",
                "\n",
                "print('Done!')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "metadata": {},
            "outputs": [],
            "source": [
                "torch.save(model.state_dict(), f\"./model/model_t4,f4,tf4.pt\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Testing trained model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 52,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Using cache found in C:\\Users\\JianQuan/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
                    ]
                }
            ],
            "source": [
                "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
                "\n",
                "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18')\n",
                "model.conv1 = nn.Conv2d(1,\n",
                "                        64,\n",
                "                        kernel_size=(7, 7),\n",
                "                        stride=(2, 2),\n",
                "                        padding=(3, 3),\n",
                "                        bias=False)\n",
                "model.load_state_dict(torch.load(\"./model/model_t4,f4,tf4.pt\"))\n",
                "\n",
                "model = model.to(device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 108,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "test: 0/837\n"
                    ]
                }
            ],
            "source": [
                "test_dataset = []\n",
                "test_dataset = load_audio_files(\"./UrbanSounds8K/test set/\", test_dataset)\n",
                "\n",
                "audio_dataloader = torch.utils.data.DataLoader(test_dataset,\n",
                "                                               batch_size=1,\n",
                "                                               shuffle=False,\n",
                "                                               num_workers=0)\n",
                "\n",
                "num_test = create_spectrogram_images(audio_dataloader, \"test\")\n",
                "\n",
                "def npy_loader(path):\n",
                "    sample = torch.from_numpy(np.load(path))\n",
                "    return sample\n",
                "\n",
                "\n",
                "spec_test_dataset = datasets.DatasetFolder(\n",
                "    root='./UrbanSounds8K/spectrograms/test/',\n",
                "    loader=npy_loader,\n",
                "    extensions=['.npy'])\n",
                "\n",
                "test_dataloader = torch.utils.data.DataLoader(\n",
                "    spec_test_dataset,\n",
                "    batch_size=64,\n",
                "    num_workers=0,\n",
                "    shuffle=False,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%ipytest\n",
                "\n",
                "def test_spectrogram_generation_test():\n",
                "    sum = 0\n",
                "    dir = [str(p) for p in Path('./UrbanSounds8K/spectrograms/test').glob('*')]\n",
                "    for path in dir:\n",
                "        num = len([str(p) for p in Path(path).glob('*')])\n",
                "        sum += num\n",
                "\n",
                "    assert sum == num_test"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 85,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(tensor([[[ 1.9782e+00, -1.5946e+00, -3.4768e+00,  ..., -3.4739e-01,\n",
                            "            3.9526e-01,  4.1237e-01],\n",
                            "          [ 1.2299e-01,  4.8282e-01,  2.8185e-02,  ..., -1.1781e-02,\n",
                            "            9.6032e-01, -1.5953e+00],\n",
                            "          [-3.3370e+00, -1.7491e+00,  2.0832e-01,  ..., -2.3191e+00,\n",
                            "           -2.1903e-01, -2.7251e+00],\n",
                            "          ...,\n",
                            "          [-2.0507e+01, -2.5278e+01, -2.7352e+01,  ..., -2.4447e+01,\n",
                            "           -2.4472e+01, -2.7562e+01],\n",
                            "          [-2.2551e+01, -2.3613e+01, -2.5990e+01,  ..., -2.9114e+01,\n",
                            "           -2.3208e+01, -3.0224e+01],\n",
                            "          [-2.6916e+01, -2.7990e+01, -3.0528e+01,  ..., -2.4957e+01,\n",
                            "           -2.3403e+01, -3.1538e+01]]]),\n",
                            " 0)"
                        ]
                    },
                    "execution_count": 85,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "next(iter(spec_test_dataset))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 132,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Test loss: 1.1381309649756592e-05\n",
                        "Test accuracy: 100.00000000000001%\n"
                    ]
                }
            ],
            "source": [
                "test_size = len(test_dataloader.dataset)\n",
                "total_batch = len(test_dataloader)\n",
                "test_loss, test_accuracy = 0, 0\n",
                "\n",
                "model.eval()\n",
                "\n",
                "cost = torch.nn.CrossEntropyLoss()\n",
                "\n",
                "with torch.no_grad():\n",
                "    for batch, (X, Y) in enumerate(test_dataloader):\n",
                "        X, Y = X.to(device), Y.to(device)\n",
                "        pred = model(X)\n",
                "        batch_loss = cost(pred, Y)\n",
                "        batch_accuracy = (pred.argmax(1) == Y).type(torch.float).sum()\n",
                "        test_loss += batch_loss.item()\n",
                "        test_accuracy += batch_accuracy.item()\n",
                "        if batch == 0:\n",
                "            first_batch_y = Y\n",
                "            first_batch_p = pred.argmax(1)\n",
                "\n",
                "\n",
                "test_loss /= test_size\n",
                "test_accuracy /= test_size/100\n",
                "\n",
                "print(f\"Test loss: {test_loss}\")\n",
                "print(f\"Test accuracy: {test_accuracy}%\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 138,
            "metadata": {},
            "outputs": [],
            "source": [
                "for i in range(len(first_batch)):\n",
                "    title\n",
                "    writer.add_audio(\n",
                "        f'Test/({test_dataset[i][2]})A:{class_map[first_batch_y[i].item()]} P:{class_map[first_batch_p[i].item()]}',\n",
                "        test_dataset[i][0], 44100)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 65,
            "metadata": {},
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'create_spectrogram_images' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn [65], line 11\u001b[0m\n\u001b[0;32m      4\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m load_audio_files(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./UrbanSounds8K/test set/\u001b[39m\u001b[38;5;124m'\u001b[39m, audio_dataset)\n\u001b[0;32m      6\u001b[0m audio_dataloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(test_dataset,\n\u001b[0;32m      7\u001b[0m                                                batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m      8\u001b[0m                                                shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m      9\u001b[0m                                                num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m \u001b[43mcreate_spectrogram_images\u001b[49m(audio_datalaoder, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, randIndex \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(randIndex_list):\n\u001b[0;32m     14\u001b[0m     waveform, class_id, title \u001b[38;5;241m=\u001b[39m test_dataset[randIndex]\n",
                        "\u001b[1;31mNameError\u001b[0m: name 'create_spectrogram_images' is not defined"
                    ]
                }
            ],
            "source": [
                "for i, randIndex in enumerate(randIndex_list):\n",
                "    waveform, class_id, title = test_dataset[randIndex]\n",
                "    spectrogram_tensor = (spectrogram(waveform) + 1e-12).log2()\n",
                "\n",
                "\n",
                "    with torch.no_grad():\n",
                "        model.eval()\n",
                "        output = model(torch.reshape(spectrogram_tensor, (-1,1)))\n",
                "    writer.add_audio(\n",
                "        f'Test/({title})A:{class_map[class_id]} P:{class_map[output]}',\n",
                "        waveform, 44100)\n",
                "\n",
                "writer.close()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "DIAP8Nvg88ag"
            },
            "source": [
                "**Debugging**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "RumP79RPD4v6",
                "outputId": "158f193d-1fbe-4000-90b8-97bb285389bc"
            },
            "outputs": [],
            "source": [
                "\n",
                "# audio_dataset = []\n",
                "# sum = 0\n",
                "\n",
                "# main_path = './UrbanSounds8K/spectrograms/'\n",
                "\n",
                "# dir = [str(p) for p in Path(main_path).glob('*')]\n",
                "# for path in dir:\n",
                "#   print('Loading: '+ path)\n",
                "#   num = len([str(p) for p in Path(path).glob('*')])\n",
                "#   sum += num\n",
                "#   print(num)\n",
                "\n",
                "# sum"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "collapsed_sections": [],
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3.10.7 64-bit (microsoft store)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.7"
        },
        "vscode": {
            "interpreter": {
                "hash": "9e87201990ab3ef9f8e8015a432b605669f23d2887f787d8acc74a222f9112d5"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
